{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-bSzzsd6O-J"
      },
      "outputs": [],
      "source": [
        "# import library for handle data\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "lFtJAzlp60gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"./\")"
      ],
      "metadata": {
        "id": "TTnR6i6z6Sef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data from the Excel file\n",
        "#data = pd.read_excel('data.xlsx')\n",
        "\n",
        "# Split the data into input (state) and output (action and reward) arrays\n",
        "state = data.iloc[:, :3].values\n",
        "action = data.iloc[:, 7:11].values\n",
        "reward = data.iloc[:, 11].values\n",
        "\n",
        "# Define the DQN model\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=3, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(4, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mse', optimizer=Adam())\n",
        "\n",
        "# Train the DQN model\n",
        "history = model.fit(state, action, epochs=100, verbose=1)\n",
        "\n",
        "# Plot the average loss over time\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Test the DQN model\n",
        "test_state = np.array([[0.4, 0.7, 0.3]])\n",
        "predicted_action = model.predict(test_state)\n",
        "print(\"Predicted action:\", predicted_action)\n",
        "\n",
        "# Evaluate the DQN model\n",
        "test_reward = np.array([0.0])\n",
        "predicted_reward = model.evaluate(test_state, test_reward, verbose=0)\n",
        "print(\"Predicted reward:\", predicted_reward)\n",
        "\n",
        "# Generate a plot of the policy\n",
        "plt.subplot(2, 2, 2)\n",
        "actions = ['Action_Express Joy', 'Action_Show Empathy', 'Action_Stay Calm', 'Action_Remain Neutral']\n",
        "probs = predicted_action[0]\n",
        "plt.bar(actions, probs)\n",
        "plt.title('Policy')\n",
        "plt.xlabel('Action')\n",
        "plt.ylabel('Probability')\n",
        "\n",
        "# Generate a plot of the agent's behavior\n",
        "plt.subplot(2, 2, 3)\n",
        "behavior = model.predict(state)\n",
        "plt.scatter(range(len(behavior)), behavior[:, 0], label='Action_Express Joy')\n",
        "plt.scatter(range(len(behavior)), behavior[:, 1], label='Action_Show Empathy')\n",
        "plt.scatter(range(len(behavior)), behavior[:, 2], label='Action_Stay Calm')\n",
        "plt.scatter(range(len(behavior)), behavior[:, 3], label='Action_Remain Neutral')\n",
        "plt.title(\"Agent's Behavior\")\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Probability')\n",
        "plt.legend()\n",
        "\n",
        "# Generate a plot of the value function\n",
        "plt.subplot(2, 2, 4)\n",
        "values = np.max(behavior, axis=1)\n",
        "plt.plot(range(len(values)), values)\n",
        "plt.title(\"Value Function\")\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot the reward vs episodes\n",
        "plt.figure(figsize=(8, 6))\n",
        "episodes = range(len(reward))\n",
        "plt.plot(episodes, reward)\n",
        "plt.title('Reward vs Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IjTmXS5A6SiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VF_Lc09q6SlH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}